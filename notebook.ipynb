{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just read in the file path to variable 'raw_dataset' using spark session or context and save the notebook as: final_year[#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then run all the commands after \"Preprocessing\" header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Spark Session --Do not run if using SparkContext\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Spark Session --Do not run if using SparkContext\n",
    "\n",
    "raw_dataset = spark.read.csv(r'filepath/year#.csv',header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark context --Do not run if using SparkSession\n",
    "raw_dataset = sqlContext.read.load('data/csv_result-1year.csv',\n",
    "                                  format='com.databricks.spark.csv',\n",
    "                                  header='true',\n",
    "                                  inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast to Type Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows: 7027\n",
      "Count of distinct rows: 7027\n",
      "Count if distinct ids: 7027\n"
     ]
    }
   ],
   "source": [
    "cols = raw_dataset.columns\n",
    "typecasted_dataset = raw_dataset.select(*(col(c).cast(\"double\").alias(c) for c in cols))\n",
    "print ('Count of rows: {0}'.format(typecasted_dataset.count()))\n",
    "print ('Count of distinct rows: {0}'.format(typecasted_dataset.distinct().count()))\n",
    "print ('Count if distinct ids: {0}'.format(typecasted_dataset.select([c for c in typecasted_dataset.columns if c!= 'id' or c!= 'class']).distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typecasted_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'Attr23_imputed',\n",
       " 'Attr20_imputed',\n",
       " 'Attr35_imputed',\n",
       " 'Attr18_imputed',\n",
       " 'Attr9_imputed',\n",
       " 'Attr17_imputed',\n",
       " 'Attr25_imputed',\n",
       " 'Attr39_imputed',\n",
       " 'Attr38_imputed',\n",
       " 'Attr15_imputed',\n",
       " 'Attr50_imputed',\n",
       " 'Attr64_imputed',\n",
       " 'Attr13_imputed',\n",
       " 'Attr42_imputed',\n",
       " 'Attr11_imputed',\n",
       " 'Attr21_imputed',\n",
       " 'Attr52_imputed',\n",
       " 'Attr56_imputed',\n",
       " 'Attr14_imputed',\n",
       " 'Attr8_imputed',\n",
       " 'Attr45_imputed',\n",
       " 'Attr26_imputed',\n",
       " 'Attr19_imputed',\n",
       " 'Attr44_imputed',\n",
       " 'Attr58_imputed',\n",
       " 'Attr34_imputed',\n",
       " 'Attr55_imputed',\n",
       " 'Attr36_imputed',\n",
       " 'Attr61_imputed',\n",
       " 'Attr3_imputed',\n",
       " 'Attr40_imputed',\n",
       " 'Attr48_imputed',\n",
       " 'Attr46_imputed',\n",
       " 'Attr29_imputed',\n",
       " 'Attr60_imputed',\n",
       " 'Attr54_imputed',\n",
       " 'Attr31_imputed',\n",
       " 'Attr43_imputed',\n",
       " 'Attr16_imputed',\n",
       " 'Attr53_imputed',\n",
       " 'Attr22_imputed',\n",
       " 'Attr12_imputed',\n",
       " 'Attr41_imputed',\n",
       " 'Attr27_imputed',\n",
       " 'Attr32_imputed',\n",
       " 'Attr28_imputed',\n",
       " 'Attr2_imputed',\n",
       " 'Attr63_imputed',\n",
       " 'Attr5_imputed',\n",
       " 'Attr4_imputed',\n",
       " 'Attr59_imputed',\n",
       " 'Attr62_imputed',\n",
       " 'Attr30_imputed',\n",
       " 'Attr24_imputed',\n",
       " 'Attr1_imputed',\n",
       " 'Attr33_imputed',\n",
       " 'Attr37_imputed',\n",
       " 'Attr7_imputed',\n",
       " 'Attr51_imputed',\n",
       " 'Attr47_imputed',\n",
       " 'Attr10_imputed',\n",
       " 'Attr57_imputed',\n",
       " 'Attr49_imputed',\n",
       " 'Attr6_imputed']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "#default is mean\n",
    "imputer = Imputer(inputCols = typecasted_dataset.columns[1:-1], outputCols = [s + \"_imputed\" for s in typecasted_dataset.columns[1:-1]])\n",
    "imp_model = imputer.fit(typecasted_dataset)\n",
    "imp_df = imp_model.transform(typecasted_dataset)\n",
    "imp_df = imp_df.select(imp_df.columns[65:])\n",
    "imp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in imp_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename column 'class' to 'label' so it works easier with spark ML functions\n",
    "imp_df = imp_df.withColumnRenamed('class','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemblerInputs = imp_df.columns[1:]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "feature_df = assembler.transform(imp_df).select(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to split the dataset into train and test before scaling\n",
    "\n",
    "train,test = feature_df.randomSplit([.7,.3],seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 4716|\n",
      "|  1.0|  180|\n",
      "+-----+-----+\n",
      "\n",
      "None\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 6756|\n",
      "|  1.0|  271|\n",
      "+-----+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.groupBy('label').count().show())\n",
    "print(feature_df.groupBy('label').count().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol = 'features',outputCol = 'scaledFeatures')\n",
    "scalerModel = scaler.fit(train)\n",
    "\n",
    "#only fit the scaler on train, not test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_train = scalerModel.transform(train).select('label','scaledFeatures')\n",
    "sc_test = scalerModel.transform(test).select('label','scaledFeatures')\n",
    "\n",
    "#transform the scaler on both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, scaledFeatures: vector]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction (PCA10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will try with 10 first, should try different # of components as well (probably fewer, maybe 3, 5 and 7 just to see which is best)\n",
    "pca = PCA(k = 10,inputCol = 'scaledFeatures', outputCol = 'pcaFeatures')\n",
    "pcaModel = pca.fit(sc_train)\n",
    "\n",
    "#only fit PCA model on train, not test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'features']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_train = pcaModel.transform(sc_train).select('label','pcaFeatures').withColumnRenamed('pcaFeatures','features')\n",
    "pca_test = pcaModel.transform(sc_test).select('label','pcaFeatures').withColumnRenamed('pcaFeatures','features')\n",
    "pca_train.columns\n",
    "\n",
    "#transform PCA model on both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction (PCA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will try with 10 first, should try different # of components as well (probably fewer, maybe 3, 5 and 7 just to see which is best)\n",
    "pca5 = PCA(k = 5,inputCol = 'scaledFeatures', outputCol = 'pcaFeatures')\n",
    "pca5Model = pca5.fit(sc_train)\n",
    "\n",
    "#only fit PCA model on train, not test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'features']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca5_train = pca5Model.transform(sc_train).select('label','pcaFeatures').withColumnRenamed('pcaFeatures','features')\n",
    "pca5_test = pca5Model.transform(sc_test).select('label','pcaFeatures').withColumnRenamed('pcaFeatures','features')\n",
    "pca5_train.columns\n",
    "\n",
    "#transform PCA model on both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca5_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMulticlassMetrics(predictions):\n",
    "    \n",
    "    #*********\n",
    "    \"\"\"\n",
    "    Function to print classification metrics. Input is a dataframe containing a column\n",
    "    with class labels named 'label' and a column with model predictions named\n",
    "    'prediction'\n",
    "    \"\"\"\n",
    "    #*********\n",
    "    \n",
    "    #define confusion matrix results based on labels and predicitons\n",
    "    tp = predictions[(predictions.label ==1) & (predictions.prediction ==1)].count()\n",
    "    tn = predictions[(predictions.label ==0) & (predictions.prediction ==0)].count()\n",
    "    fp = predictions[(predictions.label ==0) & (predictions.prediction ==1)].count()\n",
    "    fn = predictions[(predictions.label ==1) & (predictions.prediction ==0)].count()\n",
    "    \n",
    "    #print confusion matrix\n",
    "    print(\"\\n\")\n",
    "    print(\"CONFUSION MATRIX\\n\\tPredicted\\n\\t  1\\t0\\n\\t1|\",tp,\"\\t\",fn,\"|\\nActual\\t0|\",fp,\"\\t\",tn,\"|\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    #print f1 score\n",
    "    print('F1 Score:')\n",
    "    try:\n",
    "        f1 = (2*tp)/((2*tp)+fp+fn)\n",
    "        print(f1)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        \n",
    "    #print accuracy\n",
    "    print('\\nAccuracy:')\n",
    "    try:\n",
    "        acc = (tp + tn)/(tp+fn+tn+fp)\n",
    "        print(acc)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        \n",
    "    #print precision\n",
    "    print('\\nPrecision:')\n",
    "    try:\n",
    "        prec = tp / (tp + fp)\n",
    "        print(prec)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        \n",
    "    #print recall\n",
    "    print('\\nRecall:')\n",
    "    try:\n",
    "        rec = tp / (tp + fn)\n",
    "        print(rec)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        \n",
    "    e = BinaryClassificationEvaluator()\n",
    "    #print auc\n",
    "    print('\\nAUC:')\n",
    "    print(e.evaluate(predictions))\n",
    "    \n",
    "    print(\"\\n****************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from spark_stratifier import StratifiedCrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol = 'label',featuresCol = 'features')\n",
    "lr_paramGrid = ParamGridBuilder().addGrid(lr.regParam,[1.0,0.1,0.01]).addGrid(lr.elasticNetParam,[0.0,0.5,1.0]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_CrossVal = StratifiedCrossValidator(estimator = lr, estimatorParamMaps = lr_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "lr_cvModel = lr_CrossVal.fit(pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_predictions = lr_cvModel.transform(pca_train)\n",
    "lr_test_predictions = lr_cvModel.transform(pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 2 \t 178 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.02197802197802198\n",
      "\n",
      "Accuracy:\n",
      "0.9636437908496732\n",
      "\n",
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.011111111111111112\n",
      "\n",
      "AUC:\n",
      "0.5816699651305183\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Logistic Regression Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 2 \t 2038 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9563585171281088\n",
      "\n",
      "Precision:\n",
      "0.0\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5991596638655463\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Train Set Metrics\")\n",
    "printMulticlassMetrics(lr_train_predictions)\n",
    "print(\"\\n\\nLogistic Regression Test Set Metrics\")\n",
    "printMulticlassMetrics(lr_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Regularization:  0.01\n",
      "Elastic Net:  0.0\n"
     ]
    }
   ],
   "source": [
    "lr_bestModel = lr_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Regularization: \",lr_bestModel._java_obj.getRegParam())\n",
    "print(\"Elastic Net: \",lr_bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0043, -0.0263, -0.0315, 0.0237, -0.0192, 0.0444, -0.0902, 0.0385, -0.0503, 0.0406])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol = 'label',featuresCol = 'features',numTrees = 200,seed = 1)\n",
    "rf_paramGrid = ParamGridBuilder().addGrid(rf.impurity,['gini','entropy']).addGrid(rf.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_CrossVal = StratifiedCrossValidator(estimator = rf, estimatorParamMaps = rf_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "rf_cvModel = rf_CrossVal.fit(pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_predictions = rf_cvModel.transform(pca_train)\n",
    "rf_test_predictions = rf_cvModel.transform(pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 31 \t 149 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.2938388625592417\n",
      "\n",
      "Accuracy:\n",
      "0.9695669934640523\n",
      "\n",
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.17222222222222222\n",
      "\n",
      "AUC:\n",
      "0.9988143200452361\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Random Forest Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.6817146089204914\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Train Set Metrics\")\n",
    "printMulticlassMetrics(rf_train_predictions)\n",
    "print(\"\\n\\nRandom Forest Test Set Metrics\")\n",
    "printMulticlassMetrics(rf_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Impurity:  entropy\n",
      "Max Depth:  10\n"
     ]
    }
   ],
   "source": [
    "rf_bestModel = rf_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Impurity: \",rf_bestModel._java_obj.getImpurity())\n",
    "print(\"Max Depth: \",rf_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10, {0: 0.1084, 1: 0.1022, 2: 0.1341, 3: 0.0891, 4: 0.0955, 5: 0.1116, 6: 0.0889, 7: 0.0935, 8: 0.0937, 9: 0.083})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol = 'label',featuresCol = 'features',maxIter = 100,seed = 1)\n",
    "gbt_paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_CrossVal = StratifiedCrossValidator(estimator = gbt, estimatorParamMaps = gbt_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "gbt_cvModel = gbt_CrossVal.fit(pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_predictions = gbt_cvModel.transform(pca_train)\n",
    "gbt_test_predictions = gbt_cvModel.transform(pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostedTree Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 2 \t 178 |\n",
      "Actual\t0| 1 \t 4715 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.02185792349726776\n",
      "\n",
      "Accuracy:\n",
      "0.9634395424836601\n",
      "\n",
      "Precision:\n",
      "0.6666666666666666\n",
      "\n",
      "Recall:\n",
      "0.011111111111111112\n",
      "\n",
      "AUC:\n",
      "0.7779150174347343\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "GradientBoostedTree Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.6664188752424047\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"GradientBoostedTree Train Set Metrics\")\n",
    "printMulticlassMetrics(gbt_train_predictions)\n",
    "print(\"\\n\\nGradientBoostedTree Test Set Metrics\")\n",
    "printMulticlassMetrics(gbt_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Max Depth:  2\n"
     ]
    }
   ],
   "source": [
    "gbt_bestModel = gbt_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Max Depth: \",gbt_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10, {0: 0.0657, 1: 0.0832, 2: 0.2387, 3: 0.0539, 4: 0.1313, 5: 0.0928, 6: 0.0643, 7: 0.0773, 8: 0.1623, 9: 0.0305})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultilayerPerceptronClassifier(labelCol = 'label',featuresCol = 'features',maxIter = 100,seed = 1)\n",
    "mlp_paramGrid = ParamGridBuilder().addGrid(mlp.solver,['l-bfgs','gd']).addGrid(mlp.layers,[[10,5,2],[10,5,5,2],[10,5,10,2]]).build()\n",
    "#mlp.layers should be in the following format: [# of input columns, size of hidden layer 1, ...size of hidden layer N..., number of classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_CrossVal = StratifiedCrossValidator(estimator = mlp, estimatorParamMaps = mlp_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "mlp_cvModel = mlp_CrossVal.fit(pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_predictions = mlp_cvModel.transform(pca_train)\n",
    "mlp_test_predictions = mlp_cvModel.transform(pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 180 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9632352941176471\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.4558936481010234\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "MultilayerPerceptron Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.42961646196940323\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"MultilayerPerceptron Train Set Metrics\")\n",
    "printMulticlassMetrics(mlp_train_predictions)\n",
    "print(\"\\n\\nMultilayerPerceptron Test Set Metrics\")\n",
    "printMulticlassMetrics(mlp_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Layers:  [10, 5, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "mlp_bestModel = mlp_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "#print(\"Solver: \",mlp_bestModel._java_obj.getSolver()) #errors out\n",
    "print(\"Layers: \",mlp_bestModel.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models, PCA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from spark_stratifier import StratifiedCrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr5 = LogisticRegression(labelCol = 'label',featuresCol = 'features')\n",
    "lr5_paramGrid = ParamGridBuilder().addGrid(lr5.regParam,[1.0,0.1,0.01]).addGrid(lr5.elasticNetParam,[0.0,0.5,1.0]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr5_CrossVal = StratifiedCrossValidator(estimator = lr5, estimatorParamMaps = lr5_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "lr5_cvModel = lr5_CrossVal.fit(pca5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr5_train_predictions = lr5_cvModel.transform(pca5_train)\n",
    "lr5_test_predictions = lr5_cvModel.transform(pca5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 180 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9632352941176471\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5702619922721708\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Logistic Regression Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5797565179918122\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Train Set Metrics\")\n",
    "printMulticlassMetrics(lr5_train_predictions)\n",
    "print(\"\\n\\nLogistic Regression Test Set Metrics\")\n",
    "printMulticlassMetrics(lr5_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Regularization:  1.0\n",
      "Elastic Net:  0.0\n"
     ]
    }
   ],
   "source": [
    "lr5_bestModel = lr5_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Regularization: \",lr5_bestModel._java_obj.getRegParam())\n",
    "print(\"Elastic Net: \",lr5_bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0002, -0.002, 0.0005, 0.0024, -0.0047])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr5_bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf5 = RandomForestClassifier(labelCol = 'label',featuresCol = 'features',numTrees = 200,seed = 1)\n",
    "rf5_paramGrid = ParamGridBuilder().addGrid(rf5.impurity,['gini','entropy']).addGrid(rf5.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf5_CrossVal = StratifiedCrossValidator(estimator = rf5, estimatorParamMaps = rf5_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "rf5_cvModel = rf5_CrossVal.fit(pca5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf5_train_predictions = rf5_cvModel.transform(pca5_train)\n",
    "rf5_test_predictions = rf5_cvModel.transform(pca5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 180 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9632352941176471\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.56474531146923\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Random Forest Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.572382029734971\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Train Set Metrics\")\n",
    "printMulticlassMetrics(rf5_train_predictions)\n",
    "print(\"\\n\\nRandom Forest Test Set Metrics\")\n",
    "printMulticlassMetrics(rf5_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Impurity:  gini\n",
      "Max Depth:  2\n"
     ]
    }
   ],
   "source": [
    "rf5_bestModel = rf5_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Impurity: \",rf5_bestModel._java_obj.getImpurity())\n",
    "print(\"Max Depth: \",rf5_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.147, 1: 0.1554, 2: 0.3129, 3: 0.2559, 4: 0.1287})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf5_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt5 = GBTClassifier(labelCol = 'label',featuresCol = 'features',maxIter = 100,seed = 1)\n",
    "gbt5_paramGrid = ParamGridBuilder().addGrid(gbt5.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt5_CrossVal = StratifiedCrossValidator(estimator = gbt5, estimatorParamMaps = gbt5_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "gbt5_cvModel = gbt5_CrossVal.fit(pca5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt5_train_predictions = gbt5_cvModel.transform(pca5_train)\n",
    "gbt5_test_predictions = gbt5_cvModel.transform(pca5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostedTree Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 180 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9632352941176471\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.7613473046838174\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "GradientBoostedTree Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.665826330532213\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"GradientBoostedTree Train Set Metrics\")\n",
    "printMulticlassMetrics(gbt5_train_predictions)\n",
    "print(\"\\n\\nGradientBoostedTree Test Set Metrics\")\n",
    "printMulticlassMetrics(gbt5_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Max Depth:  2\n"
     ]
    }
   ],
   "source": [
    "gbt5_bestModel = gbt5_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Max Depth: \",gbt5_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.1725, 1: 0.0942, 2: 0.3217, 3: 0.2339, 4: 0.1776})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt5_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp5 = MultilayerPerceptronClassifier(labelCol = 'label',featuresCol = 'features',maxIter = 100,seed = 1)\n",
    "mlp5_paramGrid = ParamGridBuilder().addGrid(mlp5.solver,['l-bfgs','gd']).addGrid(mlp5.layers,[[5,5,2],[5,5,5,2],[5,5,10,2]]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp5_CrossVal = StratifiedCrossValidator(estimator = mlp5, estimatorParamMaps = mlp5_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "mlp5_cvModel = mlp5_CrossVal.fit(pca5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp5_train_predictions = mlp5_cvModel.transform(pca5_train)\n",
    "mlp5_test_predictions = mlp5_cvModel.transform(pca5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 180 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9632352941176471\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.31651470172462637\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "MultilayerPerceptron Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.38122710622710626\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"MultilayerPerceptron Train Set Metrics\")\n",
    "printMulticlassMetrics(mlp5_train_predictions)\n",
    "print(\"\\n\\nMultilayerPerceptron Test Set Metrics\")\n",
    "printMulticlassMetrics(mlp5_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Layers:  [5, 5, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "mlp5_bestModel = mlp5_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "#print(\"Solver: \",mlp5_bestModel._java_obj.getSolver()) #errors out\n",
    "print(\"Layers: \",mlp5_bestModel.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from spark_stratifier import StratifiedCrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsc = LogisticRegression(labelCol = 'label',featuresCol = 'scaledFeatures')\n",
    "lrsc_paramGrid = ParamGridBuilder().addGrid(lrsc.regParam,[1.0,0.1,0.01]).addGrid(lrsc.elasticNetParam,[0.0,0.5,1.0]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsc_CrossVal = StratifiedCrossValidator(estimator = lrsc, estimatorParamMaps = lrsc_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "lrsc_cvModel = lrsc_CrossVal.fit(sc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrsc_train_predictions = lrsc_cvModel.transform(sc_train)\n",
    "lrsc_test_predictions = lrsc_cvModel.transform(sc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 2 \t 178 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.02197802197802198\n",
      "\n",
      "Accuracy:\n",
      "0.9636437908496732\n",
      "\n",
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.011111111111111112\n",
      "\n",
      "AUC:\n",
      "0.5861570540005612\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Logistic Regression Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 1 \t 2039 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9568277803847959\n",
      "\n",
      "Precision:\n",
      "0.0\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5878420599008836\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Train Set Metrics\")\n",
    "printMulticlassMetrics(lrsc_train_predictions)\n",
    "print(\"\\n\\nLogistic Regression Test Set Metrics\")\n",
    "printMulticlassMetrics(lrsc_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Regularization:  0.1\n",
      "Elastic Net:  0.0\n"
     ]
    }
   ],
   "source": [
    "lrsc_bestModel = lrsc_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Regularization: \",lrsc_bestModel._java_obj.getRegParam())\n",
    "print(\"Elastic Net: \",lrsc_bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0011, -0.0015, -0.0068, -0.0061, -0.0061, -0.0039, -0.0114, -0.0004, -0.0107, 0.0148, 0.0162, -0.0069, -0.0015, -0.0003, -0.0021, -0.0045, -0.0005, 0.0015, -0.0061, -0.005, 0.0223, -0.0136, -0.0012, -0.0017, -0.0015, 0.0229, -0.0265, -0.0058, -0.0026, -0.0201, 0.0414, 0.0009, -0.0075, -0.0552, -0.0038, -0.0065, -0.0013, -0.0017, -0.0198, -0.0065, -0.0024, -0.0232, -0.0138, -0.0041, 0.0047, -0.0228, 0.0197, -0.0113, 0.0025, 0.002, 0.0004, -0.0028, -0.0018, -0.0082, -0.0023, 0.0001, 0.0008, -0.0061, 0.0189, -0.0102, -0.0106, -0.058, 0.0018, -0.0144])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrsc_bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfsc = RandomForestClassifier(labelCol = 'label',featuresCol = 'scaledFeatures',numTrees = 200,seed = 1)\n",
    "rfsc_paramGrid = ParamGridBuilder().addGrid(rfsc.impurity,['gini','entropy']).addGrid(rfsc.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfsc_CrossVal = StratifiedCrossValidator(estimator = rfsc, estimatorParamMaps = rfsc_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "rfsc_cvModel = rfsc_CrossVal.fit(sc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfsc_train_predictions = rfsc_cvModel.transform(sc_train)\n",
    "rfsc_test_predictions = rfsc_cvModel.transform(sc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 127 \t 53 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.8273615635179153\n",
      "\n",
      "Accuracy:\n",
      "0.9891748366013072\n",
      "\n",
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.7055555555555556\n",
      "\n",
      "AUC:\n",
      "0.9999858637263217\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Random Forest Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 25 \t 66 |\n",
      "Actual\t0| 1 \t 2039 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.42735042735042733\n",
      "\n",
      "Accuracy:\n",
      "0.9685593618019709\n",
      "\n",
      "Precision:\n",
      "0.9615384615384616\n",
      "\n",
      "Recall:\n",
      "0.27472527472527475\n",
      "\n",
      "AUC:\n",
      "0.9063833225597934\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Train Set Metrics\")\n",
    "printMulticlassMetrics(rfsc_train_predictions)\n",
    "print(\"\\n\\nRandom Forest Test Set Metrics\")\n",
    "printMulticlassMetrics(rfsc_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Impurity:  gini\n",
      "Max Depth:  10\n"
     ]
    }
   ],
   "source": [
    "rfsc_bestModel = rfsc_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Impurity: \",rfsc_bestModel._java_obj.getImpurity())\n",
    "print(\"Max Depth: \",rfsc_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(64, {0: 0.0167, 1: 0.0138, 2: 0.0138, 3: 0.008, 4: 0.0303, 5: 0.006, 6: 0.0109, 7: 0.0104, 8: 0.0099, 9: 0.0098, 10: 0.0125, 11: 0.0098, 12: 0.0119, 13: 0.0106, 14: 0.0528, 15: 0.0152, 16: 0.0094, 17: 0.0204, 18: 0.0089, 19: 0.0097, 20: 0.0098, 21: 0.0122, 22: 0.0109, 23: 0.0121, 24: 0.0252, 25: 0.0404, 26: 0.0148, 27: 0.0125, 28: 0.0125, 29: 0.0117, 30: 0.0192, 31: 0.0126, 32: 0.0253, 33: 0.0259, 34: 0.0106, 35: 0.0139, 36: 0.0106, 37: 0.0103, 38: 0.0123, 39: 0.011, 40: 0.0165, 41: 0.0113, 42: 0.0117, 43: 0.1356, 44: 0.0099, 45: 0.0117, 46: 0.0074, 47: 0.0076, 48: 0.0118, 49: 0.012, 50: 0.0065, 51: 0.0093, 52: 0.0119, 53: 0.0191, 54: 0.0122, 55: 0.01, 56: 0.0104, 57: 0.0091, 58: 0.0096, 59: 0.0111, 60: 0.0078, 61: 0.0085, 62: 0.0133, 63: 0.0212})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfsc_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtsc = GBTClassifier(labelCol = 'label',featuresCol = 'scaledFeatures',maxIter = 100,seed = 1)\n",
    "gbtsc_paramGrid = ParamGridBuilder().addGrid(gbtsc.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtsc_CrossVal = StratifiedCrossValidator(estimator = gbtsc, estimatorParamMaps = gbtsc_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "gbtsc_cvModel = gbtsc_CrossVal.fit(sc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtsc_train_predictions = gbtsc_cvModel.transform(sc_train)\n",
    "gbtsc_test_predictions = gbtsc_cvModel.transform(sc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostedTree Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 90 \t 90 |\n",
      "Actual\t0| 5 \t 4711 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.6545454545454545\n",
      "\n",
      "Accuracy:\n",
      "0.9805964052287581\n",
      "\n",
      "Precision:\n",
      "0.9473684210526315\n",
      "\n",
      "Recall:\n",
      "0.5\n",
      "\n",
      "AUC:\n",
      "0.9528961690698261\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "GradientBoostedTree Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 42 \t 49 |\n",
      "Actual\t0| 1 \t 2039 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.6268656716417911\n",
      "\n",
      "Accuracy:\n",
      "0.97653683716565\n",
      "\n",
      "Precision:\n",
      "0.9767441860465116\n",
      "\n",
      "Recall:\n",
      "0.46153846153846156\n",
      "\n",
      "AUC:\n",
      "0.888100624865331\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"GradientBoostedTree Train Set Metrics\")\n",
    "printMulticlassMetrics(gbtsc_train_predictions)\n",
    "print(\"\\n\\nGradientBoostedTree Test Set Metrics\")\n",
    "printMulticlassMetrics(gbtsc_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Max Depth:  2\n"
     ]
    }
   ],
   "source": [
    "gbtsc_bestModel = gbtsc_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Max Depth: \",gbtsc_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(64, {1: 0.0099, 4: 0.023, 6: 0.0145, 7: 0.0156, 8: 0.0334, 9: 0.0147, 12: 0.0465, 13: 0.009, 14: 0.1628, 15: 0.067, 17: 0.0277, 21: 0.0052, 24: 0.0111, 25: 0.0954, 30: 0.0173, 31: 0.0007, 32: 0.0442, 33: 0.0261, 34: 0.0044, 37: 0.0048, 40: 0.0066, 42: 0.0086, 43: 0.1479, 45: 0.0106, 47: 0.0043, 48: 0.02, 51: 0.0042, 53: 0.0463, 55: 0.0347, 56: 0.0021, 58: 0.0251, 60: 0.0011, 62: 0.0257, 63: 0.0294})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtsc_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpsc = MultilayerPerceptronClassifier(labelCol = 'label',featuresCol = 'scaledFeatures',maxIter = 100,seed = 1)\n",
    "mlpsc_paramGrid = ParamGridBuilder().addGrid(mlpsc.solver,['l-bfgs','gd']).addGrid(mlpsc.layers,[[64,5,2],[64,5,5,2],[64,5,10,2]]).build()\n",
    "#mlpsc.layers should be in the following format: [# of input columns, size of hidden layer 1, ...size of hidden layer N..., number of classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpsc_CrossVal = StratifiedCrossValidator(estimator = mlpsc, estimatorParamMaps = mlpsc_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "mlpsc_cvModel = mlpsc_CrossVal.fit(sc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpsc_train_predictions = mlpsc_cvModel.transform(sc_train)\n",
    "mlpsc_test_predictions = mlpsc_cvModel.transform(sc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 180 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9632352941176471\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5441134200358114\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "MultilayerPerceptron Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5343191122602888\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"MultilayerPerceptron Train Set Metrics\")\n",
    "printMulticlassMetrics(mlpsc_train_predictions)\n",
    "print(\"\\n\\nMultilayerPerceptron Test Set Metrics\")\n",
    "printMulticlassMetrics(mlpsc_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Layers:  [64, 5, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "mlpsc_bestModel = mlpsc_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "#print(\"Solver: \",mlpsc_bestModel._java_obj.getSolver()) #errors out\n",
    "print(\"Layers: \",mlpsc_bestModel.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models without PCA or Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from spark_stratifier import StratifiedCrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base = LogisticRegression(labelCol = 'label',featuresCol = 'features')\n",
    "lr_base_paramGrid = ParamGridBuilder().addGrid(lr_base.regParam,[1.0,0.1,0.01]).addGrid(lr_base.elasticNetParam,[0.0,0.5,1.0]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base_CrossVal = StratifiedCrossValidator(estimator = lr_base, estimatorParamMaps = lr_base_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "lr_base_cvModel = lr_base_CrossVal.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrtrain_predictions = lr_base_cvModel.transform(train)\n",
    "lrtest_predictions = lr_base_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 2 \t 178 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.02197802197802198\n",
      "\n",
      "Accuracy:\n",
      "0.9636437908496732\n",
      "\n",
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.011111111111111112\n",
      "\n",
      "AUC:\n",
      "0.5861570540005612\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Logistic Regression Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 1 \t 2039 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9568277803847959\n",
      "\n",
      "Precision:\n",
      "0.0\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5878420599008836\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Train Set Metrics\")\n",
    "printMulticlassMetrics(lrtrain_predictions)\n",
    "print(\"\\n\\nLogistic Regression Test Set Metrics\")\n",
    "printMulticlassMetrics(lrtest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Regularization:  0.1\n",
      "Elastic Net:  0.0\n"
     ]
    }
   ],
   "source": [
    "lr_base_bestModel = lr_base_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Regularization: \",lr_base_bestModel._java_obj.getRegParam())\n",
    "print(\"Elastic Net: \",lr_base_bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0, -0.0, -0.0012, -0.0009, -0.0001, -0.0002, -0.0003, -0.0, -0.0003, 0.0, 0.0029, -0.0, -0.0, -0.0, -0.0003, -0.0, -0.0, 0.0, -0.0009, -0.0002, 0.0002, -0.0015, -0.0, -0.0, -0.0, 0.0013, -0.0, -0.0001, -0.0, -0.0032, 0.0114, 0.0001, -0.0017, -0.0816, -0.0, -0.0, -0.0, -0.0, -0.003, -0.0, -0.0003, -0.0042, -0.0009, -0.0, 0.0, -0.0007, 0.0031, -0.0005, 0.0, 0.0004, 0.0001, -0.0, -0.0, -0.0009, -0.0004, 0.0, 0.0, -0.0009, 0.003, -0.0, -0.0003, -0.0124, 0.0, -0.002])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_base_bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier(labelCol = 'label',featuresCol = 'features',numTrees = 200,seed = 1)\n",
    "rf_base_paramGrid = ParamGridBuilder().addGrid(rf_base.impurity,['gini','entropy']).addGrid(rf_base.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base_CrossVal = StratifiedCrossValidator(estimator = rf_base, estimatorParamMaps = rf_base_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "rf_base_cvModel = rf_base_CrossVal.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rftrain_predictions = rf_base_cvModel.transform(train)\n",
    "rftest_predictions = rf_base_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 127 \t 53 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.8273615635179153\n",
      "\n",
      "Accuracy:\n",
      "0.9891748366013072\n",
      "\n",
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.7055555555555556\n",
      "\n",
      "AUC:\n",
      "0.9999858637263217\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "Random Forest Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 25 \t 66 |\n",
      "Actual\t0| 1 \t 2039 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.42735042735042733\n",
      "\n",
      "Accuracy:\n",
      "0.9685593618019709\n",
      "\n",
      "Precision:\n",
      "0.9615384615384616\n",
      "\n",
      "Recall:\n",
      "0.27472527472527475\n",
      "\n",
      "AUC:\n",
      "0.9063833225597934\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Train Set Metrics\")\n",
    "printMulticlassMetrics(rftrain_predictions)\n",
    "print(\"\\n\\nRandom Forest Test Set Metrics\")\n",
    "printMulticlassMetrics(rftest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Impurity:  gini\n",
      "Max Depth:  10\n"
     ]
    }
   ],
   "source": [
    "rf_base_bestModel = rf_base_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Impurity: \",rf_base_bestModel._java_obj.getImpurity())\n",
    "print(\"Max Depth: \",rf_base_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(64, {0: 0.0167, 1: 0.0138, 2: 0.0138, 3: 0.008, 4: 0.0303, 5: 0.006, 6: 0.0109, 7: 0.0104, 8: 0.0099, 9: 0.0098, 10: 0.0125, 11: 0.0098, 12: 0.0119, 13: 0.0106, 14: 0.0528, 15: 0.0152, 16: 0.0094, 17: 0.0204, 18: 0.0089, 19: 0.0097, 20: 0.0098, 21: 0.0122, 22: 0.0109, 23: 0.0121, 24: 0.0252, 25: 0.0404, 26: 0.0148, 27: 0.0125, 28: 0.0125, 29: 0.0117, 30: 0.0192, 31: 0.0126, 32: 0.0253, 33: 0.0259, 34: 0.0106, 35: 0.0139, 36: 0.0106, 37: 0.0103, 38: 0.0123, 39: 0.011, 40: 0.0165, 41: 0.0113, 42: 0.0117, 43: 0.1356, 44: 0.0099, 45: 0.0117, 46: 0.0074, 47: 0.0076, 48: 0.0118, 49: 0.012, 50: 0.0065, 51: 0.0093, 52: 0.0119, 53: 0.0191, 54: 0.0122, 55: 0.01, 56: 0.0104, 57: 0.0091, 58: 0.0096, 59: 0.0111, 60: 0.0078, 61: 0.0085, 62: 0.0133, 63: 0.0212})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_base_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_base = GBTClassifier(labelCol = 'label',featuresCol = 'features',maxIter = 100,seed = 1)\n",
    "gbt_base_paramGrid = ParamGridBuilder().addGrid(gbt_base.maxDepth,[2,5,10]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_base_CrossVal = StratifiedCrossValidator(estimator = gbt_base, estimatorParamMaps = gbt_base_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "gbt_base_cvModel = gbt_base_CrossVal.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbttrain_predictions = gbt_base_cvModel.transform(train)\n",
    "gbttest_predictions = gbt_base_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostedTree Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 173 \t 7 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.9801699716713881\n",
      "\n",
      "Accuracy:\n",
      "0.9985702614379085\n",
      "\n",
      "Precision:\n",
      "1.0\n",
      "\n",
      "Recall:\n",
      "0.9611111111111111\n",
      "\n",
      "AUC:\n",
      "0.999997643954387\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "GradientBoostedTree Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 45 \t 46 |\n",
      "Actual\t0| 9 \t 2031 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.6206896551724138\n",
      "\n",
      "Accuracy:\n",
      "0.9741905208822149\n",
      "\n",
      "Precision:\n",
      "0.8333333333333334\n",
      "\n",
      "Recall:\n",
      "0.4945054945054945\n",
      "\n",
      "AUC:\n",
      "0.9332390648567122\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"GradientBoostedTree Train Set Metrics\")\n",
    "printMulticlassMetrics(gbttrain_predictions)\n",
    "print(\"\\n\\nGradientBoostedTree Test Set Metrics\")\n",
    "printMulticlassMetrics(gbttest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Max Depth:  5\n"
     ]
    }
   ],
   "source": [
    "gbt_base_bestModel = gbt_base_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "print(\"Max Depth: \",gbt_base_bestModel._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(64, {0: 0.0268, 1: 0.0151, 2: 0.0132, 3: 0.0139, 4: 0.0325, 5: 0.0199, 6: 0.0213, 7: 0.0129, 8: 0.0082, 9: 0.0151, 10: 0.012, 11: 0.0142, 12: 0.0143, 13: 0.0031, 14: 0.0334, 15: 0.0214, 16: 0.0068, 17: 0.0393, 19: 0.0083, 20: 0.0095, 21: 0.009, 22: 0.002, 23: 0.0106, 24: 0.0655, 25: 0.0755, 26: 0.0222, 27: 0.0074, 28: 0.011, 29: 0.0071, 30: 0.0168, 31: 0.0034, 32: 0.061, 33: 0.0318, 34: 0.0049, 35: 0.0075, 36: 0.0082, 37: 0.0037, 38: 0.0042, 39: 0.0099, 40: 0.0028, 41: 0.0043, 42: 0.0177, 43: 0.0606, 44: 0.006, 45: 0.0086, 46: 0.0065, 47: 0.0035, 48: 0.066, 49: 0.0101, 50: 0.01, 51: 0.0022, 52: 0.0105, 53: 0.0124, 54: 0.0066, 55: 0.0041, 56: 0.0157, 58: 0.004, 59: 0.0033, 60: 0.0035, 61: 0.0084, 62: 0.0114, 63: 0.0191})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_base_bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_base = MultilayerPerceptronClassifier(labelCol = 'label',featuresCol = 'features',maxIter = 100,seed = 1)\n",
    "mlp_base_paramGrid = ParamGridBuilder().addGrid(mlp_base.solver,['l-bfgs','gd']).addGrid(mlp_base.layers,[[64,5,2],[64,5,5,2],[64,5,10,2]]).build()\n",
    "#mlp_base.layers should be in the following format: [# of input columns, size of hidden layer 1, ...size of hidden layer N..., number of classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_base_CrossVal = StratifiedCrossValidator(estimator = mlp_base, estimatorParamMaps = mlp_base_paramGrid,evaluator = MulticlassClassificationEvaluator(),numFolds = 10)\n",
    "mlp_base_cvModel = mlp_base_CrossVal.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlptrain_predictions = mlp_base_cvModel.transform(train)\n",
    "mlptest_predictions = mlp_base_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron Train Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 180 |\n",
      "Actual\t0| 0 \t 4716 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9632352941176471\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5598724201300528\n",
      "\n",
      "****************************************************************\n",
      "\n",
      "\n",
      "MultilayerPerceptron Test Set Metrics\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\tPredicted\n",
      "\t  1\t0\n",
      "\t1| 0 \t 91 |\n",
      "Actual\t0| 0 \t 2040 |\n",
      "\n",
      "\n",
      "F1 Score:\n",
      "0.0\n",
      "\n",
      "Accuracy:\n",
      "0.9572970436414828\n",
      "\n",
      "Precision:\n",
      "error\n",
      "\n",
      "Recall:\n",
      "0.0\n",
      "\n",
      "AUC:\n",
      "0.5325495582848524\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"MultilayerPerceptron Train Set Metrics\")\n",
    "printMulticlassMetrics(mlptrain_predictions)\n",
    "print(\"\\n\\nMultilayerPerceptron Test Set Metrics\")\n",
    "printMulticlassMetrics(mlptest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics\n",
      "Layers:  [64, 5, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "mlp_base_bestModel = mlp_base_cvModel.bestModel\n",
    "print(\"Best Metrics\")\n",
    "#print(\"Solver: \",mlp_base_bestModel._java_obj.getSolver()) #errors out\n",
    "print(\"Layers: \",mlp_base_bestModel.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See linked ExcelOnline Sheet. Enter results in manually to the correct section so we can compare our results between models and between years"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
